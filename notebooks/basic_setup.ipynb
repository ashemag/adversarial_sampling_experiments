{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Make sure that all relevant paths are added to system path so that modules can be imported correctly\n",
    "- Make sure relevant paths are added to system path (so modules can be imported correctly).\n",
    "- Make sure that \"../adversarial_sampling_experiments/data/mnist-train.npz\" exists.\n",
    "- Training the network that will be used for carrying out basic experiments/ testing functionality of code. (takes 20 seconds.)\n",
    "    - Before training make sure that all folders and files references actually exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding relevant paths to system path\n",
    "To access all modules we have to add to system path the folder above above \"adversarial_sampling_experiments\". On my system this is the \"mlp_new\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory:  C:\\mlp_new\\adversarial_sampling_experiments\n",
      "one directory up:  C:\\mlp_new\n",
      "added root dir to sys.path:  C:\\mlp_new\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "one_up_dir = str(Path(os.getcwd()).parents[0]) # you have to cast to string otherwise it won't work.\n",
    "print(\"current directory: \",os.getcwd())\n",
    "print(\"one directory up: \",one_up_dir)\n",
    "\n",
    "if one_up_dir not in sys.path:\n",
    "    sys.path.append(one_up_dir)\n",
    "    print(\"added root dir to sys.path: \",one_up_dir)\n",
    "else:\n",
    "    print(\"root dir: \",one_up_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use CPU\n",
      "building feed-forward network module\n",
      "torch.Size([2, 1, 28, 28]) input\n",
      "torch.Size([2, 100]) fc-relu\n",
      "torch.Size([2, 10]) fc\n",
      "100it [00:00, 321.32it/s]\n",
      "OrderedDict([('current_epoch', 0), ('train_acc', 0.7154), ('train_loss', 1.2404), ('epoch_train_time', 0.3142077922821045)])\n",
      "100it [00:00, 353.11it/s]\n",
      "OrderedDict([('current_epoch', 1), ('train_acc', 0.8654), ('train_loss', 0.5063), ('epoch_train_time', 0.28620147705078125)])\n",
      "100it [00:00, 354.36it/s]\n",
      "OrderedDict([('current_epoch', 2), ('train_acc', 0.8816), ('train_loss', 0.4158), ('epoch_train_time', 0.2852015495300293)])\n",
      "100it [00:00, 330.89it/s]\n",
      "OrderedDict([('current_epoch', 3), ('train_acc', 0.8984), ('train_loss', 0.3621), ('epoch_train_time', 0.30521655082702637)])\n",
      "100it [00:00, 343.40it/s]\n",
      "OrderedDict([('current_epoch', 4), ('train_acc', 0.9008), ('train_loss', 0.3409), ('epoch_train_time', 0.2952079772949219)])\n",
      "100it [00:00, 353.11it/s]\n",
      "OrderedDict([('current_epoch', 5), ('train_acc', 0.9127), ('train_loss', 0.3078), ('epoch_train_time', 0.28620171546936035)])\n",
      "100it [00:00, 345.78it/s]\n",
      "OrderedDict([('current_epoch', 6), ('train_acc', 0.9131), ('train_loss', 0.3085), ('epoch_train_time', 0.2922070026397705)])\n",
      "100it [00:00, 341.06it/s]\n",
      "OrderedDict([('current_epoch', 7), ('train_acc', 0.9189), ('train_loss', 0.2908), ('epoch_train_time', 0.29621028900146484)])\n",
      "100it [00:00, 346.98it/s]\n",
      "OrderedDict([('current_epoch', 8), ('train_acc', 0.9185), ('train_loss', 0.2774), ('epoch_train_time', 0.29120588302612305)])\n",
      "100it [00:00, 351.86it/s]\n",
      "OrderedDict([('current_epoch', 9), ('train_acc', 0.9198), ('train_loss', 0.2785), ('epoch_train_time', 0.28820323944091797)])\n",
      "100it [00:00, 336.46it/s]\n",
      "OrderedDict([('current_epoch', 10), ('train_acc', 0.9281), ('train_loss', 0.2586), ('epoch_train_time', 0.30021214485168457)])\n",
      "100it [00:00, 331.99it/s]\n",
      "OrderedDict([('current_epoch', 11), ('train_acc', 0.9306), ('train_loss', 0.2421), ('epoch_train_time', 0.30521655082702637)])\n",
      "100it [00:00, 337.60it/s]\n",
      "OrderedDict([('current_epoch', 12), ('train_acc', 0.9304), ('train_loss', 0.2403), ('epoch_train_time', 0.29821085929870605)])\n",
      "100it [00:00, 339.90it/s]\n",
      "OrderedDict([('current_epoch', 13), ('train_acc', 0.9356), ('train_loss', 0.2277), ('epoch_train_time', 0.29821085929870605)])\n",
      "100it [00:00, 344.58it/s]\n",
      "OrderedDict([('current_epoch', 14), ('train_acc', 0.9417), ('train_loss', 0.2121), ('epoch_train_time', 0.29320716857910156)])\n",
      "100it [00:00, 339.90it/s]\n",
      "OrderedDict([('current_epoch', 15), ('train_acc', 0.9364), ('train_loss', 0.2207), ('epoch_train_time', 0.2972099781036377)])\n",
      "100it [00:00, 329.80it/s]\n",
      "OrderedDict([('current_epoch', 16), ('train_acc', 0.9414), ('train_loss', 0.2102), ('epoch_train_time', 0.3062169551849365)])\n",
      "100it [00:00, 325.50it/s]\n",
      "OrderedDict([('current_epoch', 17), ('train_acc', 0.943), ('train_loss', 0.195), ('epoch_train_time', 0.311220645904541)])\n",
      "100it [00:00, 327.64it/s]\n",
      "OrderedDict([('current_epoch', 18), ('train_acc', 0.9457), ('train_loss', 0.1915), ('epoch_train_time', 0.30821752548217773)])\n",
      "100it [00:00, 327.64it/s]\n",
      "OrderedDict([('current_epoch', 19), ('train_acc', 0.9451), ('train_loss', 0.1913), ('epoch_train_time', 0.30821847915649414)])\n",
      "100it [00:00, 344.58it/s]\n",
      "OrderedDict([('current_epoch', 20), ('train_acc', 0.9487), ('train_loss', 0.1859), ('epoch_train_time', 0.2952086925506592)])\n",
      "100it [00:00, 336.46it/s]\n",
      "OrderedDict([('current_epoch', 21), ('train_acc', 0.949), ('train_loss', 0.183), ('epoch_train_time', 0.30021166801452637)])\n",
      "100it [00:00, 337.60it/s]\n",
      "OrderedDict([('current_epoch', 22), ('train_acc', 0.9532), ('train_loss', 0.1683), ('epoch_train_time', 0.29821085929870605)])\n",
      "100it [00:00, 341.05it/s]\n",
      "OrderedDict([('current_epoch', 23), ('train_acc', 0.9533), ('train_loss', 0.169), ('epoch_train_time', 0.29620933532714844)])\n",
      "100it [00:00, 309.38it/s]\n",
      "OrderedDict([('current_epoch', 24), ('train_acc', 0.9518), ('train_loss', 0.1742), ('epoch_train_time', 0.32723212242126465)])\n",
      "100it [00:00, 336.46it/s]\n",
      "OrderedDict([('current_epoch', 25), ('train_acc', 0.9527), ('train_loss', 0.1689), ('epoch_train_time', 0.3002126216888428)])\n",
      "100it [00:00, 338.74it/s]\n",
      "OrderedDict([('current_epoch', 26), ('train_acc', 0.9575), ('train_loss', 0.1563), ('epoch_train_time', 0.2992110252380371)])\n",
      "100it [00:00, 327.64it/s]\n",
      "OrderedDict([('current_epoch', 27), ('train_acc', 0.9571), ('train_loss', 0.1499), ('epoch_train_time', 0.30821824073791504)])\n",
      "100it [00:00, 346.98it/s]\n",
      "OrderedDict([('current_epoch', 28), ('train_acc', 0.9615), ('train_loss', 0.1392), ('epoch_train_time', 0.29120492935180664)])\n",
      "100it [00:00, 343.40it/s]\n",
      "OrderedDict([('current_epoch', 29), ('train_acc', 0.9578), ('train_loss', 0.1453), ('epoch_train_time', 0.2942075729370117)])\n",
      "100it [00:00, 338.75it/s]\n",
      "OrderedDict([('current_epoch', 30), ('train_acc', 0.9592), ('train_loss', 0.14), ('epoch_train_time', 0.2972097396850586)])\n",
      "100it [00:00, 339.90it/s]\n",
      "OrderedDict([('current_epoch', 31), ('train_acc', 0.9607), ('train_loss', 0.1369), ('epoch_train_time', 0.29821038246154785)])\n",
      "100it [00:00, 339.89it/s]\n",
      "OrderedDict([('current_epoch', 32), ('train_acc', 0.9591), ('train_loss', 0.1421), ('epoch_train_time', 0.2972087860107422)])\n",
      "100it [00:00, 323.40it/s]\n",
      "OrderedDict([('current_epoch', 33), ('train_acc', 0.9618), ('train_loss', 0.1351), ('epoch_train_time', 0.3112201690673828)])\n",
      "100it [00:00, 335.33it/s]\n",
      "OrderedDict([('current_epoch', 34), ('train_acc', 0.9643), ('train_loss', 0.1295), ('epoch_train_time', 0.30121374130249023)])\n",
      "100it [00:00, 322.35it/s]\n",
      "OrderedDict([('current_epoch', 35), ('train_acc', 0.9631), ('train_loss', 0.1278), ('epoch_train_time', 0.3132212162017822)])\n",
      "100it [00:00, 343.40it/s]\n",
      "OrderedDict([('current_epoch', 36), ('train_acc', 0.9664), ('train_loss', 0.1196), ('epoch_train_time', 0.2952079772949219)])\n",
      "100it [00:00, 341.06it/s]\n",
      "OrderedDict([('current_epoch', 37), ('train_acc', 0.965), ('train_loss', 0.1222), ('epoch_train_time', 0.2952086925506592)])\n",
      "100it [00:00, 349.40it/s]\n",
      "OrderedDict([('current_epoch', 38), ('train_acc', 0.9666), ('train_loss', 0.1192), ('epoch_train_time', 0.28820371627807617)])\n",
      "100it [00:00, 318.25it/s]\n",
      "OrderedDict([('current_epoch', 39), ('train_acc', 0.9666), ('train_loss', 0.1171), ('epoch_train_time', 0.31722521781921387)])\n",
      "100it [00:00, 326.57it/s]\n",
      "OrderedDict([('current_epoch', 40), ('train_acc', 0.9663), ('train_loss', 0.1248), ('epoch_train_time', 0.31021952629089355)])\n",
      "100it [00:00, 315.24it/s]\n",
      "OrderedDict([('current_epoch', 41), ('train_acc', 0.9667), ('train_loss', 0.118), ('epoch_train_time', 0.3192253112792969)])\n",
      "100it [00:00, 325.50it/s]\n",
      "OrderedDict([('current_epoch', 42), ('train_acc', 0.9688), ('train_loss', 0.1097), ('epoch_train_time', 0.3092186450958252)])\n",
      "100it [00:00, 336.46it/s]\n",
      "OrderedDict([('current_epoch', 43), ('train_acc', 0.9697), ('train_loss', 0.1162), ('epoch_train_time', 0.3002126216888428)])\n",
      "100it [00:00, 339.90it/s]\n",
      "OrderedDict([('current_epoch', 44), ('train_acc', 0.9672), ('train_loss', 0.1064), ('epoch_train_time', 0.2972097396850586)])\n",
      "100it [00:00, 353.11it/s]\n",
      "OrderedDict([('current_epoch', 45), ('train_acc', 0.9696), ('train_loss', 0.1053), ('epoch_train_time', 0.2872035503387451)])\n",
      "100it [00:00, 339.90it/s]\n",
      "OrderedDict([('current_epoch', 46), ('train_acc', 0.9701), ('train_loss', 0.1065), ('epoch_train_time', 0.2972097396850586)])\n",
      "100it [00:00, 337.60it/s]\n",
      "OrderedDict([('current_epoch', 47), ('train_acc', 0.9714), ('train_loss', 0.1027), ('epoch_train_time', 0.29821085929870605)])\n",
      "100it [00:00, 336.46it/s]\n",
      "OrderedDict([('current_epoch', 48), ('train_acc', 0.973), ('train_loss', 0.1004), ('epoch_train_time', 0.3012125492095947)])\n",
      "100it [00:00, 317.24it/s]\n",
      "OrderedDict([('current_epoch', 49), ('train_acc', 0.9718), ('train_loss', 0.0963), ('epoch_train_time', 0.31722426414489746)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nremarks:\\n(1) why not use the DataProvider to get the MNIST data? answer: DataProvider is an iterator. I think it's\\ncleaner to have a function that returns the data as it is, in a specific format that is the same for all\\ndatasets. ImageDataGetter will always return x, y as numpy arrays. \\nwith x shape: (batch_size, num_channels, height, width), and y shape: (batch_size,) (integer encoded true\\nlabels)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adversarial_sampling_experiments.data_providers import DataProvider\n",
    "from adversarial_sampling_experiments.data_providers import ImageDataGetter\n",
    "from adversarial_sampling_experiments.models.simple_fnn import FeedForwardNetwork\n",
    "from adversarial_sampling_experiments.globals import ROOT_DIR\n",
    "from torch import optim\n",
    "    \n",
    "model = FeedForwardNetwork(img_shape=(1, 28, 28), num_classes=10)\n",
    "x, y = ImageDataGetter.mnist(filename=os.path.join(ROOT_DIR,'data/mnist-train.npz')) # (1)\n",
    "train_data_iterator = DataProvider(x,y,batch_size=100,max_num_batches=100,make_one_hot=False,rng=None)\n",
    "\n",
    "model.train_full(\n",
    "    train_data=train_data_iterator,\n",
    "    num_epochs=50,\n",
    "    optimizer=optim.SGD(model.parameters(), lr=1e-1),\n",
    "    train_file_path=os.path.join(ROOT_DIR, 'ExperimentResults/simple_fnn/train_results.txt'),\n",
    "    model_save_dir=os.path.join(ROOT_DIR, 'saved_models/simple_fnn'),\n",
    "    integer_encoded = True\n",
    ")\n",
    "\n",
    "'''\n",
    "remarks:\n",
    "(1) why not use the DataProvider to get the MNIST data? answer: DataProvider is an iterator. I think it's\n",
    "cleaner to have a function that returns the data as it is, in a specific format that is the same for all\n",
    "datasets. ImageDataGetter will always return x, y as numpy arrays. \n",
    "with x shape: (batch_size, num_channels, height, width), and y shape: (batch_size,) (integer encoded true\n",
    "labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
